#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê°œì„ ëœ ë ˆì‹œí”¼ í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸
ë§Œê°œì˜ ë ˆì‹œí”¼ì—ì„œ 100ê°œ ë ˆì‹œí”¼ë¥¼ í¬ë¡¤ë§í•˜ê³  Firebaseì— ì—…ë¡œë“œ
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import random
import re
import os
import sys
from urllib.parse import urljoin, urlparse
import firebase_admin
from firebase_admin import credentials, firestore

# Firebase ì´ˆê¸°í™”
def initialize_firebase():
    """Firebase Admin SDK ì´ˆê¸°í™”"""
    try:
        credential_path = './firebase-credentials.json'
        
        if not os.path.exists(credential_path):
            raise FileNotFoundError("firebase-credentials.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"âœ… Firebase ì¸ì¦ì„œ íŒŒì¼ ë°œê²¬: {credential_path}")
        
        # Firebase Admin SDK ì´ˆê¸°í™”
        cred = credentials.Certificate(credential_path)
        firebase_admin.initialize_app(cred)
        
        # Firestore í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        db = firestore.client()
        print("âœ… Firebase ì´ˆê¸°í™” ì™„ë£Œ")
        return db
        
    except Exception as e:
        print(f"âŒ Firebase ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None

# í¬ë¡¤ë§ ì„¤ì •
BASE_URL = "https://www.10000recipe.com"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}

# ìš”ë¦¬ë³„ ê²€ìƒ‰ í‚¤ì›Œë“œ (ê°ê° ë‹¤ë¥¸ ìš”ë¦¬ë¡œ ì·¨ê¸‰)
RECIPE_KEYWORDS = [
    # í•œì‹ (ìš°ì„  ê°€ì¤‘ì¹˜ ëŒ€ìƒ)
    "ê¹€ì¹˜ì°Œê°œ", "ëœì¥ì°Œê°œ", "ê³ ì¶”ì¥ì°Œê°œ", "ë¹„ë¹”ë°¥", "ë¶ˆê³ ê¸°", "ê°„ì¥ë¶ˆê³ ê¸°", "ê³ ì¶”ì¥ë¶ˆê³ ê¸°", "ê°ˆë¹„ì°œ", "ì–‘ë…ê°ˆë¹„", "ì‚¼ê²¹ì‚´", "ì‚¼ê²¹ì‚´êµ¬ì´",
    "ë‹­ë³¶ìŒíƒ•", "ë‹­ê°ˆë¹„", "ë–¡ë³¶ì´", "ê°„ì¥ë–¡ë³¶ì´", "ìˆœë‘ë¶€ì°Œê°œ", "ê¹€ì¹˜ì „", "íŒŒì „", "ë¶€ì¹¨ê°œ", "ê³„ë€ë§ì´", "ë©¸ì¹˜ë³¶ìŒ",
    "ì‹œë˜ê¸°êµ­", "ì½©ë‚˜ë¬¼êµ­", "ë¯¸ì—­êµ­", "ëœì¥êµ­", "ê¹€ì¹˜ë³¶ìŒë°¥", "ê³„ë€ë³¶ìŒë°¥", "ì¡ì±„", "ìœ¡ê°œì¥", "ì„¤ë íƒ•", "ê°ˆë¹„íƒ•",
    "ì‚¼ê³„íƒ•", "ë‹­ë³¶ìŒ", "ì˜¤ì§•ì–´ë³¶ìŒ", "ë‚™ì§€ë³¶ìŒ", "ê³ ë“±ì–´ì¡°ë¦¼", "ê°ˆì¹˜ì¡°ë¦¼", "ìƒì„ êµ¬ì´", "ë¼ë©´", "ëƒ‰ë©´", "ë§Œë‘", "êµì", "ë³¶ìŒìš°ë™", "ê¹€ì¹˜ì°œ",

    # ì–‘ì‹ (ìƒëŸ¬ë“œëŠ” 2ì¢…ë§Œ, íŒŒìŠ¤íƒ€ ë‹¤ì–‘í™”)
    "í† ë§ˆí† íŒŒìŠ¤íƒ€", "í¬ë¦¼íŒŒìŠ¤íƒ€", "ì˜¤ì¼íŒŒìŠ¤íƒ€", "ë´‰ê³¨ë ˆíŒŒìŠ¤íƒ€", "ì•Œë¦¬ì˜¤ì˜¬ë¦¬ì˜¤", "ê°ë°”ìŠ¤", "ê¹Œë¥´ë³´ë‚˜ë¼", "ë¼ìëƒ", "ë¦¬ì¡°ë˜",
    "í”¼ì", "ë§ˆë¥´ê²Œë¦¬íƒ€í”¼ì", "í˜í¼ë¡œë‹ˆí”¼ì", "í–„ë²„ê±°", "ì¹˜ì¦ˆë²„ê±°", "ìŠ¤í…Œì´í¬", "ê·¸ë¼íƒ•", "ì˜¤ë¯ˆë ›", "ìŠ¤í¬ë¨ë¸”",
    "íŒ¬ì¼€ì´í¬", "ì™€í”Œ", "ìƒŒë“œìœ„ì¹˜", "í† ìŠ¤íŠ¸", "ë¸ŒëŸ°ì¹˜", "ì‹œì €ìƒëŸ¬ë“œ", "ê·¸ë¦°ìƒëŸ¬ë“œ",

    # ì¼ì‹ (ì¶”ê°€: ì¹´ë ˆ, ìƒ¤ë¸Œìƒ¤ë¸Œ, ë°€í‘€ìœ ë‚˜ë² )
    "ê³„ë€ì´ˆë°¥", "ë¼ë©˜", "ìš°ë™", "ì†Œë°”", "ëˆì¹´ì¸ ", "í…í‘¸ë¼", "ì•¼í‚¤ì†Œë°”", "ì˜¤ì½”ë…¸ë¯¸ì•¼í‚¤", "íƒ€ì½”ì•¼í‚¤", "ê·œë™",
    "ê°€ì¸ ë™", "ì˜¤ì•¼ì½”ë™", "ì¹˜í‚¨ê°€ë¼ì•„ê²Œ", "ì‚¬ì‹œë¯¸", "ìŠ¤ì‹œ", "ì¹´ë ˆ", "ìƒ¤ë¸Œìƒ¤ë¸Œ", "ë°€í‘€ìœ ë‚˜ë² ",

    # ì¤‘ì‹ (ê²¹ì¹˜ëŠ” í•œì‹ í‚¤ì›Œë“œ ì œê±°, ìƒˆë¡œìš´ ìš”ë¦¬ ì¶”ê°€)
    "ì§œì¥ë©´", "ì§¬ë½•", "íƒ•ìˆ˜ìœ¡", "ê¹í’ê¸°", "ë§ˆíŒŒë‘ë¶€", "ê¿”ë°”ë¡œìš°", "ì¶˜ê¶Œ", "ë”¤ì„¬", "ìƒ¤ì˜¤ë¡±ë°”ì˜¤", "ì–´í–¥ê°€ì§€",
    "ê°€ì§€íŠ€ê¹€", "ë§ˆë¼íƒ•", "ë§ˆë¼ìƒ¹ê¶ˆ", "ê³ ê¸°ë”¤ì„¬"
]

def get_session():
    """ì„¸ì…˜ ìƒì„±"""
    session = requests.Session()
    session.headers.update(HEADERS)
    return session

def search_recipes(session, keyword, max_pages=2):
    """í‚¤ì›Œë“œë¡œ ë ˆì‹œí”¼ ê²€ìƒ‰"""
    recipe_urls = []
    
    for page in range(1, max_pages + 1):
        try:
            search_url = f"{BASE_URL}/recipe/list.html?q={keyword}&order=reco&page={page}"
            print(f"    ğŸ” ê²€ìƒ‰ ì¤‘: {keyword} (í˜ì´ì§€ {page})")
            
            response = session.get(search_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë ˆì‹œí”¼ ë§í¬ ì¶”ì¶œ
            recipe_links = soup.find_all('a', href=re.compile(r'/recipe/\d+'))
            
            for link in recipe_links:
                href = link.get('href')
                if href and '/recipe/' in href:
                    full_url = urljoin(BASE_URL, href)
                    if full_url not in recipe_urls:
                        recipe_urls.append(full_url)
            
            print(f"    ğŸ“‹ {len(recipe_links)}ê°œ ë ˆì‹œí”¼ ë§í¬ ë°œê²¬ (ì´ {len(recipe_urls)}ê°œ)")
            
            time.sleep(random.uniform(1, 2))
            
        except Exception as e:
            print(f"    âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            continue
    
    return recipe_urls

def clean_ingredient_text(text):
    """ì¬ë£Œ í…ìŠ¤íŠ¸ ì •ë¦¬ (êµ¬ë§¤/ì¡°ë¦¬ë„êµ¬ ì œê±°)"""
    # "êµ¬ë§¤" ì œê±°
    text = re.sub(r'êµ¬ë§¤$', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    # ì¡°ë¦¬ë„êµ¬ í‚¤ì›Œë“œ ì œê±°
    utensil_keywords = [
        'ë„ë§ˆ','ì¹¼','ì¡°ë¦¬ìš©ë‚˜ì´í”„','ë‚˜ì´í”„','ìŠ¤í‘¼','ìˆ˜ì €','ìˆŸê°€ë½','ì “ê°€ë½','ì§‘ê²Œ','ë’¤ì§‘ê°œ','êµ­ì','ê±°í’ˆê¸°','ë³¼','ê·¸ë¦‡',
        'ëƒ„ë¹„','íŒ¬','í”„ë¼ì´íŒ¬','ì˜¤ë¸','ì „ìë ˆì¸ì§€','ë¯¹ì„œê¸°','ë¸”ë Œë”','ì²´','ë§','ì°œê¸°','ì••ë ¥ì†¥','ê³„ëŸ‰ì»µ','ê³„ëŸ‰ìŠ¤í‘¼','ìš”ë¦¬ë„êµ¬','ì¡°ë¦¬ë„êµ¬', 'ëšë°°ê¸°'
    ]
    lowered = text.lower()
    for k in utensil_keywords:
        if k in lowered:
            return ''
    return text

def extract_servings(soup):
    """ëª‡ì¸ë¶„ì¸ì§€ ì¶”ì¶œ"""
    try:
        # ì—¬ëŸ¬ íŒ¨í„´ìœ¼ë¡œ ì¸ë¶„ ì •ë³´ ì°¾ê¸°
        patterns = [
            r'(\d+)ì¸ë¶„',
            r'(\d+)ì¸',
            r'(\d+)ëª…',
            r'(\d+)ì¸ìš©'
        ]
        
        text_content = soup.get_text()
        
        for pattern in patterns:
            match = re.search(pattern, text_content)
            if match:
                return int(match.group(1))
        
        # ê¸°ë³¸ê°’
        return random.randint(1, 4)
        
    except:
        return random.randint(1, 4)

def clean_recipe_steps(steps):
    """ë ˆì‹œí”¼ ë‹¨ê³„ ì •ë¦¬ (ë¶ˆí•„ìš”í•œ ë‚´ìš© ì œê±°, ê°„ê²°í™”)"""
    cleaned_steps = []
    remove_prefix_patterns = [r'^[0-9]+\.\s*', r'^[ê°€-í£A-Za-z]*\s*:']
    ban_words = ['ì´ì•¼ê¸°', 'í›„ê¸°', 'ì†Œê°', 'ì¡ë‹´', 'ê´‘ê³ ', 'ì´ë²¤íŠ¸', 'íŒë§¤', 'êµ¬ë…', 'ì¢‹ì•„ìš”', 'ëŒ“ê¸€', 'ê³µìœ ']
    tip_words = ['íŒ', 'TIP', 'ì°¸ê³ ', 'ì£¼ì˜', 'ë…¸í•˜ìš°']

    for step in steps:
        if not step:
            continue
        # ë„ˆë¬´ ì§§ì€ ë‹¨ê³„ ì œê±° (5ê¸€ì ë¯¸ë§Œ)
        if len(step) < 5:
            continue

        # ë²ˆí˜¸/ë¼ë²¨ ì œê±°
        for pat in remove_prefix_patterns:
            step = re.sub(pat, '', step)
        # ê´„í˜¸/ëŒ€ê´„í˜¸ëŠ” ë”ì´ìƒ ì œê±°í•˜ì§€ ì•ŠìŒ (ë³´ì¡°ì„¤ëª… ìœ ì§€)
        # íŒ/ì£¼ì„ ì„¹ì…˜ ì˜ë¼ë‚´ê¸°
        for w in tip_words:
            idx = step.find(w)
            if idx != -1 and idx > 0:
                step = step[:idx]
                break
        # ê´‘ê³ /ì¡ë‹´ í¬í•¨ ë‹¨ê³„ ì œì™¸
        if any(w in step for w in ban_words):
            continue
        # ê³µë°± ì •ë¦¬
        step = re.sub(r'\s+', ' ', step).strip()
        # ì´ë¯¸ ë‹¨ê³„ ì¶”ì¶œì—ì„œ ë³´ì¡°ì„¤ëª…ì„ ê´„í˜¸ë¡œ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ ì¶”ê°€ ì²˜ë¦¬ ë¶ˆí•„ìš”
        # ë„ˆë¬´ ê¸´ ë¬¸ì¥ì€ ìë¥´ê¸° (ë³´ì¡°ì„¤ëª…ì´ í¬í•¨ë˜ë¯€ë¡œ ì œí•œì„ ëŠ˜ë¦¼)
        if len(step) > 300:
            step = step[:300].rstrip() + 'â€¦'
        if step and len(step) >= 5:
            cleaned_steps.append(step)

    return cleaned_steps

def extract_recipe_data(session, url):
    """ë ˆì‹œí”¼ ë°ì´í„° ì¶”ì¶œ"""
    try:
        # ëª¨ë°”ì¼ ë„ë©”ì¸ìœ¼ë¡œ ë“¤ì–´ì˜¨ ê²½ìš° ë°ìŠ¤í¬í†± ë„ë©”ì¸ìœ¼ë¡œ ì •ê·œí™”
        if url.startswith('https://m.10000recipe.com'):
            url = url.replace('https://m.10000recipe.com', BASE_URL)
        response = session.get(url, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ë ˆì‹œí”¼ ì œëª©
        title_elem = soup.find('h3', class_='view2_summary_info3') or soup.find('h1', class_='view2_summary_info3')
        if not title_elem:
            title_elem = soup.find('h3') or soup.find('h1')
        
        title = title_elem.get_text(strip=True) if title_elem else "ì œëª© ì—†ìŒ"
        # ë°€í‚¤íŠ¸ ë ˆì‹œí”¼ ì œì™¸
        if 'ë°€í‚¤íŠ¸' in title:
            return None
        
        # ë ˆì‹œí”¼ ì´ë¯¸ì§€ (ë§Œê°œì˜ ë ˆì‹œí”¼ ì´ë¯¸ì§€ë§Œ)
        img_elem = soup.find('img', class_='view2_summary_img') or soup.find('img', {'class': re.compile(r'.*summary_img.*')})
        if not img_elem:
            img_elem = soup.find('img', {'src': re.compile(r'.*recipe.*')}) or soup.find('img', {'src': re.compile(r'.*food.*')})
        
        image_url = ""
        if img_elem and img_elem.get('src'):
            img_src = img_elem.get('src')
            if img_src.startswith('//'):
                image_url = 'https:' + img_src
            elif img_src.startswith('/'):
                image_url = BASE_URL + img_src
            elif img_src.startswith('http'):
                image_url = img_src
        
        # ì¬ë£Œ ì •ë³´
        ingredients = []
        ingredient_sections = soup.find_all('div', class_='ready_ingre3')
        
        for section in ingredient_sections:
            ingredient_items = section.find_all('li')
            for item in ingredient_items:
                ingredient_text = item.get_text(strip=True)
                if ingredient_text and ingredient_text != 'ì¬ë£Œ':
                    # ì¬ë£Œ í…ìŠ¤íŠ¸ ì •ë¦¬
                    cleaned_ingredient = clean_ingredient_text(ingredient_text)
                    # ë°€í‚¤íŠ¸ ì–¸ê¸‰ í¬í•¨ ì‹œ ì œì™¸
                    if cleaned_ingredient and 'ë°€í‚¤íŠ¸' not in cleaned_ingredient:
                        ingredients.append(cleaned_ingredient)
        
        # ì¡°ë¦¬ë²•
        steps = []
        # 1ìˆœìœ„: ë°ìŠ¤í¬í†±ì˜ ëª…í™•í•œ ë‹¨ê³„ ë¸”ë¡(id=stepDivN)
        step_items = soup.find_all('div', id=re.compile(r'^stepDiv\d+$'))
        # ëŒ€ì²´: ë„“ì€ ì„ íƒì (ëª¨ë°”ì¼/ë³€í˜• ëŒ€ì‘)
        if not step_items:
            step_sections = soup.select('div.view_step, section.view_step, div.rd_step, section.rd_step, [class*="step"]')
            step_items = []
            for section in step_sections:
                step_items.extend(section.select('div.view_step_cont, div.rd_step_cont, li.view_step_cont, .step_cont, .step_txt, li'))
        
        for item in step_items:
            # 1) ë©”ì¸ í…ìŠ¤íŠ¸ì™€ ë³´ì¡°ì„¤ëª…ì„ HTML êµ¬ì¡°ë¡œ ë¶„ë¦¬
            main_text = ''
            sub_text = ''
            
            # ë©”ì¸ í…ìŠ¤íŠ¸: media-bodyì—ì„œ step_add í´ë˜ìŠ¤ ì œì™¸í•œ ë¶€ë¶„
            main_elem = item.find('div', class_='media-body')
            if main_elem:
                # step_add í´ë˜ìŠ¤ ìš”ì†Œë“¤ ì œê±°í•˜ê³  ë©”ì¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                main_elem_copy = main_elem.__copy__()
                for tip_elem in main_elem_copy.find_all('p', class_='step_add'):
                    tip_elem.decompose()
                main_text = main_elem_copy.get_text('\n', strip=True)
                # ëª¨ë“  ì¼ë°˜ ì¤„ì„ ë©”ì¸ í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš© (ì¤„ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ)
                main_lines = [ln.strip() for ln in main_text.split('\n') if ln.strip()]
                main_text = ' '.join(main_lines) if main_lines else ''
            
            # ë³´ì¡°ì„¤ëª…: step_add í´ë˜ìŠ¤ ìš”ì†Œë“¤
            tip_elements = item.find_all('p', class_='step_add')
            tip_texts = []
            for tip_elem in tip_elements:
                tip_text = tip_elem.get_text(' ', strip=True)
                if tip_text:
                    tip_texts.append(tip_text)
            
            if tip_texts:
                sub_text = ' '.join(tip_texts)
            
            # 2) ë¶ˆë¦¿(â€¢)ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë³´ì¡°ì„¤ëª…ë„ ì°¾ê¸°
            raw_text = item.get_text('\n', strip=True)
            lines = [ln.strip() for ln in raw_text.split('\n') if ln and ln.strip()]
            
            bullet_lines = []
            for line in lines:
                if re.match(r'^[â€¢ã†â—â˜…â˜†â—†â—‡â–ªâ–«]\s*', line):
                    bullet_text = re.sub(r'^[â€¢ã†â—â˜…â˜†â—†â—‡â–ªâ–«]+\s*', '', line).strip()
                    if bullet_text:
                        bullet_lines.append(bullet_text)
            
            if bullet_lines:
                bullet_text = ' '.join(bullet_lines)
                sub_text = (sub_text + ' ' + bullet_text).strip() if sub_text else bullet_text
            
            # 3) ì‘ì€ íšŒìƒ‰/ë³´ì¡° span ë“¤ ìˆ˜ì§‘í•˜ì—¬ ë³´ì¡°ì„¤ëª…ì— ì¶”ê°€
            sub_elements = item.find_all('span', class_=re.compile(r'(small|gray|tip|note)', re.I))
            extra_subs = [se.get_text(' ', strip=True) for se in sub_elements if se.get_text(strip=True)]
            if extra_subs:
                sub_text = (sub_text + ' ' + ' '.join(extra_subs)).strip() if sub_text else ' '.join(extra_subs)
            
            # 4) ê²°í•© ë° ì •ë¦¬
            step_text = main_text.strip()
            if step_text and sub_text:
                step_text = f"{step_text} ({sub_text})"
            elif not step_text and sub_text:
                step_text = sub_text
            elif not step_text:
                # ë©”ì¸ í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš©
                step_text = ' '.join([ln.strip() for ln in lines if ln.strip()])
            
            step_text = re.sub(r'\s+', ' ', step_text).strip()
            
            if step_text and 'ë°€í‚¤íŠ¸' not in step_text:
                steps.append(step_text)
        
        # ì¡°ë¦¬ë²• ì •ë¦¬
        steps = clean_recipe_steps(steps)
        
        # íƒœê·¸
        tags = []
        tag_elements = soup.find_all('a', href=re.compile(r'/recipe/list\.html\?q='))
        for tag_elem in tag_elements:
            tag_text = tag_elem.get_text(strip=True)
            if tag_text and len(tag_text) > 1:
                tags.append(tag_text)
        
        # ì¸ë¶„ ìˆ˜ ì¶”ì¶œ
        servings = extract_servings(soup)
        
        # ê¸°ë³¸ ì •ë³´
        recipe_id = f"crawled_{int(time.time())}_{random.randint(1000, 9999)}"
        
        # ìš”ë¦¬ ì¹´í…Œê³ ë¦¬ íŒë‹¨
        category = "í•œì‹"
        if any(keyword in title for keyword in ["íŒŒìŠ¤íƒ€", "ìŠ¤íŒŒê²Œí‹°", "í”¼ì", "ìŠ¤í…Œì´í¬", "ìƒëŸ¬ë“œ", "ê·¸ë¼íƒ•", "ì˜¤ë¯ˆë ›"]):
            category = "ì–‘ì‹"
        elif any(keyword in title for keyword in ["ì´ˆë°¥", "ë¼ë©˜", "ìš°ë™", "ëˆì¹´ì¸ ", "í…í‘¸ë¼", "ê·œë™", "ê°€ì¸ ë™"]):
            category = "ì¼ì‹"
        elif any(keyword in title for keyword in ["ì§œì¥ë©´", "ì§¬ë½•", "íƒ•ìˆ˜ìœ¡", "ê¹í’ê¸°", "ë§ˆíŒŒë‘ë¶€", "ì¶˜ê¶Œ", "ë§Œë‘"]):
            category = "ì¤‘ì‹"
        
        recipe_data = {
            "id": recipe_id,
            "name": title,
            "ingredients": ingredients,
            "steps": steps,
            "tags": tags[:5],  # ìµœëŒ€ 5ê°œ íƒœê·¸
            "imageUrl": image_url,
            "sourceUrl": url,
            "category": category,
            "servings": servings,
            "cookingTime": random.randint(15, 120),  # 15-120ë¶„
            "difficulty": random.choice(["ì‰¬ì›€", "ë³´í†µ", "ì–´ë ¤ì›€"]),
            "createdAt": firestore.SERVER_TIMESTAMP
        }
        
        return recipe_data
        
    except Exception as e:
        print(f"    âŒ ë ˆì‹œí”¼ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None

def score_recipe(recipe_data):
    """ë ˆì‹œí”¼ ì ìˆ˜ ê³„ì‚° (ê°„ë‹¨/ëª…ë£Œ ì¤‘ì‹¬)"""
    score = 0

    name = recipe_data.get('name', '')
    ingredients = recipe_data.get('ingredients', [])
    steps = recipe_data.get('steps', [])

    # ê°„ë‹¨/ì´ˆê°„ë‹¨/ì›íŒ¬/ìì·¨ ë“± í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤
    keyword_bonus = 0
    for kw, pts in [("ì´ˆê°„ë‹¨", 25), ("ê°„ë‹¨", 15), ("ì›íŒ¬", 10), ("ìì·¨", 10), ("í•œê·¸ë¦‡", 8), ("ë¹ ë¥¸", 8), ("ì‰½ê²Œ", 8)]:
        if kw in name:
            keyword_bonus += pts
    score += min(keyword_bonus, 40)

    # ë°±ì¢…ì›/ë°±ì„ ìƒì€ ì†Œí­ ê°€ì‚°ì ë§Œ
    if ('ë°±ì¢…ì›' in name) or ('ë°±ì„ ìƒ' in name):
        score += 10

    # ì¬ë£Œ ìˆ˜ê°€ ì ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜ (ìµœëŒ€ 50ì )
    ingredient_count = len(ingredients)
    if ingredient_count <= 4:
        score += 50
    elif ingredient_count <= 6:
        score += 40
    elif ingredient_count <= 9:
        score += 25
    elif ingredient_count <= 12:
        score += 15
    else:
        score += 5

    # ì¡°ë¦¬ë²• ë‹¨ê³„ê°€ ì ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜ (ìµœëŒ€ 40ì )
    step_count = len(steps)
    if step_count <= 3:
        score += 40
    elif step_count <= 5:
        score += 30
    elif step_count <= 8:
        score += 15

    # ë‹¨ê³„ í‰ê·  ê¸¸ì´ê°€ ì§§ì„ìˆ˜ë¡ ê°€ì‚°ì  (ìµœëŒ€ 20ì )
    if step_count > 0:
        avg_len = sum(len(s) for s in steps) / step_count
        if avg_len <= 60:
            score += 20
        elif avg_len <= 100:
            score += 12
        elif avg_len <= 140:
            score += 6

    # ì§€ë‚˜ì¹˜ê²Œ ê¸´ ë‹¨ê³„ê°€ ìˆìœ¼ë©´ ê°ì 
    if any(len(s) > 260 for s in steps):
        score -= 10

    # ê¸°ë³¸ ì ìˆ˜
    score += 10

    return score

def crawl_recipes():
    """ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜"""
    print("ğŸš€ ë ˆì‹œí”¼ í¬ë¡¤ë§ ì‹œì‘")
    
    # Firebase ì´ˆê¸°í™”
    db = initialize_firebase()
    if not db:
        return
    
    session = get_session()
    all_recipes = []
    selected_recipes = {}  # ìš”ë¦¬ë³„ë¡œ ìµœê³  ì ìˆ˜ ë ˆì‹œí”¼ë§Œ ì €ì¥
    
    print(f"\nğŸ“ {len(RECIPE_KEYWORDS)}ê°œ ìš”ë¦¬ í‚¤ì›Œë“œë¡œ í¬ë¡¤ë§ ì‹œì‘...")
    
    for i, keyword in enumerate(RECIPE_KEYWORDS):
        print(f"\nğŸ“ {i+1}/{len(RECIPE_KEYWORDS)}: '{keyword}' ê²€ìƒ‰ ì¤‘...")
        
        # í•´ë‹¹ í‚¤ì›Œë“œë¡œ ë ˆì‹œí”¼ ê²€ìƒ‰
        recipe_urls = search_recipes(session, keyword, max_pages=2)
        
        if not recipe_urls:
            print(f"    âš ï¸ '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            continue
        
        # ê° ë ˆì‹œí”¼ í¬ë¡¤ë§í•˜ì—¬ ì ìˆ˜ ê³„ì‚°
        keyword_recipes = []
        
        for url in recipe_urls[:10]:  # í‚¤ì›Œë“œë‹¹ ìµœëŒ€ 10ê°œ í¬ë¡¤ë§
            print(f"    ğŸ“ ë ˆì‹œí”¼ ìˆ˜ì§‘ ì¤‘...")
            recipe_data = extract_recipe_data(session, url)
            
            if recipe_data and len(recipe_data['ingredients']) >= 3 and len(recipe_data['steps']) >= 2:
                recipe_data['score'] = score_recipe(recipe_data)
                keyword_recipes.append(recipe_data)
                print(f"    âœ… ìˆ˜ì§‘ ì™„ë£Œ: {recipe_data['name']} (ì ìˆ˜: {recipe_data['score']})")
            else:
                print(f"    âš ï¸ ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ìŠ¤í‚µ")
            
            time.sleep(random.uniform(1, 2))
        
        # í•´ë‹¹ í‚¤ì›Œë“œì˜ ìµœê³  ì ìˆ˜ ë ˆì‹œí”¼ ì„ íƒ
        if keyword_recipes:
            best_recipe = max(keyword_recipes, key=lambda x: x['score'])
            selected_recipes[keyword] = best_recipe
            print(f"    ğŸ† '{keyword}' ìµœê³  ì ìˆ˜ ë ˆì‹œí”¼ ì„ íƒ: {best_recipe['name']} (ì ìˆ˜: {best_recipe['score']})")
        else:
            print(f"    âŒ '{keyword}' ìœ íš¨í•œ ë ˆì‹œí”¼ ì—†ìŒ")
    
    # ì„ íƒëœ ë ˆì‹œí”¼ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    all_recipes = list(selected_recipes.values())
    
    print(f"\nğŸ“Š í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_recipes)}ê°œ ë ˆì‹œí”¼ ì„ íƒ")
    
    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
    category_count = {}
    for recipe in all_recipes:
        category = recipe['category']
        category_count[category] = category_count.get(category, 0) + 1
    
    print("ğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ í†µê³„:")
    for category, count in category_count.items():
        print(f"   - {category}: {count}ê°œ")
    
    # Firebaseì— ì—…ë¡œë“œ
    print(f"\nğŸ”¥ Firebase ì—…ë¡œë“œ ì‹œì‘...")
    
    try:
        # ê¸°ì¡´ ë ˆì‹œí”¼ ëª¨ë‘ ì‚­ì œ
        print("ğŸ—‘ï¸ ê¸°ì¡´ ë ˆì‹œí”¼ ì‚­ì œ ì¤‘...")
        recipes_ref = db.collection('recipes')
        docs = recipes_ref.stream()
        for doc in docs:
            doc.reference.delete()
        print("âœ… ê¸°ì¡´ ë ˆì‹œí”¼ ì‚­ì œ ì™„ë£Œ")
        
        # ìƒˆ ë ˆì‹œí”¼ ì—…ë¡œë“œ
        print("ğŸ“¤ ìƒˆ ë ˆì‹œí”¼ ì—…ë¡œë“œ ì¤‘...")
        for i, recipe in enumerate(all_recipes):
            try:
                # score í•„ë“œ ì œê±° (Firebaseì— ì €ì¥í•˜ì§€ ì•ŠìŒ)
                recipe_to_upload = {k: v for k, v in recipe.items() if k != 'score'}
                
                recipes_ref.document(recipe['id']).set(recipe_to_upload)
                print(f"    âœ… {i+1}/{len(all_recipes)}: {recipe['name']} ({recipe['category']}, {recipe['servings']}ì¸ë¶„)")
                
            except Exception as e:
                print(f"    âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {recipe['name']} - {e}")
        
        print(f"\nğŸ‰ ì™„ë£Œ! {len(all_recipes)}ê°œ ë ˆì‹œí”¼ê°€ Firebaseì— ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ Firebase ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    crawl_recipes()
